{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09580bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==1.3.4\n",
    "!pip install transformers==4.12.5\n",
    "!pip install datasets==1.15.1\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import ClassLabel\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f37120",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mlc = torch.load(os.path.join(\"/notebooks/Notebooks/general_notebooks/datasets\", 'pe_dataset_for_MLC.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a5aa12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'essay_nr', 'starting_idx', 'component_label', 'ending_idx', 'text', 'link_label', 'split', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'label', 'structural_fts_as_text', 'joined_label', 'strct_fts_w_linked', 'mc', 'cl', 'prem', 'link'],\n",
       "        num_rows: 3769\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'essay_nr', 'starting_idx', 'component_label', 'ending_idx', 'text', 'link_label', 'split', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'label', 'structural_fts_as_text', 'joined_label', 'strct_fts_w_linked', 'mc', 'cl', 'prem', 'link'],\n",
       "        num_rows: 1138\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', 'essay_nr', 'starting_idx', 'component_label', 'ending_idx', 'text', 'link_label', 'split', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'label', 'structural_fts_as_text', 'joined_label', 'strct_fts_w_linked', 'mc', 'cl', 'prem', 'link'],\n",
       "        num_rows: 943\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df5375fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do we need: text 'mc', 'cl', 'prem', 'link'\n",
    "# that's enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090a64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mlc = dataset_mlc.remove_columns(['Unnamed: 0', 'essay_nr', 'starting_idx', 'component_label', 'ending_idx', 'link_label', 'split', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'label', 'joined_label', 'strct_fts_w_linked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01ecd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'structural_fts_as_text', 'mc', 'cl', 'prem', 'link'],\n",
       "        num_rows: 3769\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'structural_fts_as_text', 'mc', 'cl', 'prem', 'link'],\n",
       "        num_rows: 1138\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'structural_fts_as_text', 'mc', 'cl', 'prem', 'link'],\n",
       "        num_rows: 943\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c2b65a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'criminal minds that breed violence not stringent gun control',\n",
       " 'structural_fts_as_text': 'Topic: Gun control and increasing violence, Sentence: To some extent, I do not agree with this assertion because I believe that criminal minds that breed violence not stringent gun control, Para Number: 1, First in Para: Yes, Last in Para: Yes, Is in Introduction: Yes, Is in Conclusion: No',\n",
       " 'mc': 1,\n",
       " 'cl': 0,\n",
       " 'prem': 0,\n",
       " 'link': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mlc['train'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31891201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mc', 'cl', 'prem', 'link']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset_mlc['train'].features.keys() if label not in ['text', 'structural_fts_as_text']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "540a4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "#from transformers import Trainer\n",
    "from transformers import AutoTokenizer\n",
    "#from transformers import BertForSequenceClassification\n",
    "#from transformers import Trainer, TrainingArguments\n",
    "#from transformers.data.data_collator import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52b16d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"structural_fts_as_text\"]\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256)\n",
    "    # add labels\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    # fill numpy array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "        encoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a3b3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_data at 0x7f7be9a91790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd47350a0e8345a0b5f93a72b7bd81ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d66dd20b2d04524865110557f27d45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1ec8d0c7a64af480050f4154d61c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset_mlc.map(preprocess_data, batched=True, remove_columns=dataset_mlc['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f0802c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0enAb0W9o25W",
    "outputId": "55bc5ba6-d169-49c6-f562-bb7ea4143866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['attention_mask', 'input_ids', 'labels', 'token_type_ids'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "690a6568",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "D0McCtJ8HRJY",
    "outputId": "82fb0336-51a3-40ad-ebc0-65eeb7cf4b6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] topic : gender equality at university admission, sentence : they want female candidates for soft natured work like counseling, teaching, designing etc, para number : 3, first in para : no, last in para : no, is in introduction : no, is in conclusion : no [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3cc1a44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdIvj6WjHeZQ",
    "outputId": "418c14d2-cca3-44e9-d0a2-6ad4ca7a007c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 1.0, 0.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8924d27d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4Dx95t2o6N9",
    "outputId": "3ce6c923-0b45-4743-bc4b-7771d088b03e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prem']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5d4721a",
   "metadata": {
    "id": "Lk6Cq9duKBkA"
   },
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe9723a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "961dba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd155a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2225d323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "571fece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "metric_name = \"f1\"\n",
    "nr_epochs = 6\n",
    "results_folder = \"/notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9f80dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \n",
    "    output_dir=results_folder,          \n",
    "    \n",
    "    # params\n",
    "    num_train_epochs=nr_epochs,               # nb of epochs\n",
    "    per_device_train_batch_size=batch_size,   # batch size per device during training\n",
    "    per_device_eval_batch_size=batch_size,    # cf. paper Sun et al.\n",
    "    learning_rate=1e-5,#2e-5,                 # cf. paper Sun et al.\n",
    "#     warmup_steps=500,                         # number of warmup steps for learning rate scheduler\n",
    "    warmup_ratio=0.1,                         # cf. paper Sun et al.\n",
    "    weight_decay=0.01,                        # strength of weight decay\n",
    "    \n",
    "    # eval\n",
    "    evaluation_strategy=\"steps\",              # cf. paper Sun et al.\n",
    "    eval_steps=20,                            # cf. paper Sun et al.\n",
    "    \n",
    "    # log\n",
    "#     logging_dir=\"/notebooks/Results/bert_sequence_classification/tb_logs\",  \n",
    "#     logging_strategy='steps',\n",
    "#     logging_steps=20,\n",
    "    \n",
    "    # save\n",
    "    save_strategy='steps',\n",
    "    save_total_limit=2,\n",
    "    # save_steps=20, # default 500\n",
    "    load_best_model_at_end=True,              # cf. paper Sun et al.\n",
    "    # metric_for_best_model='eval_loss' \n",
    "    metric_for_best_model=metric_name\n",
    "    \n",
    "    \n",
    "    \n",
    "#     f\"bert-finetuned-pe-mlc\",\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "#     save_strategy = \"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=metric_name,\n",
    "#     #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f195586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_macro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33494edc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IlOgGiojuWwG",
    "outputId": "cd0b2c99-b520-468d-8ffc-c36211b7820a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cec32ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y41Kre_jvD7x",
    "outputId": "b6ca888b-6371-40fb-ab83-3dc24d28320a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  8476,  1024,  5907,  9945,  2012,  2118,  9634,  1010,  6251,\n",
       "         1024,  2027,  2215,  2931,  5347,  2005,  3730,  3267,  2094,  2147,\n",
       "         2066, 17041,  1010,  4252,  1010, 12697,  4385,  1010, 11498,  2193,\n",
       "         1024,  1017,  1010,  2034,  1999, 11498,  1024,  2053,  1010,  2197,\n",
       "         1999, 11498,  1024,  2053,  1010,  2003,  1999,  4955,  1024,  2053,\n",
       "         1010,  2003,  1999,  7091,  1024,  2053,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a59bbd19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxWcnZ8ku12V",
    "outputId": "26522911-c3cd-466a-ae2d-d81d23003c23"
   },
   "outputs": [],
   "source": [
    "#forward pass\n",
    "# outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50e55f0a",
   "metadata": {
    "id": "chq_3nUz73ib"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "023e4c50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KXmFds8js6P8",
    "outputId": "66ebb2ab-f93f-48aa-a4dc-36f55f2f8559"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3769\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1416' max='1416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1416/1416 24:03, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.663298</td>\n",
       "      <td>0.291411</td>\n",
       "      <td>0.605123</td>\n",
       "      <td>0.183457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.605629</td>\n",
       "      <td>0.197342</td>\n",
       "      <td>0.695544</td>\n",
       "      <td>0.559915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.553885</td>\n",
       "      <td>0.195010</td>\n",
       "      <td>0.692099</td>\n",
       "      <td>0.559915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.523985</td>\n",
       "      <td>0.193433</td>\n",
       "      <td>0.689850</td>\n",
       "      <td>0.560976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.518359</td>\n",
       "      <td>0.193559</td>\n",
       "      <td>0.690041</td>\n",
       "      <td>0.560976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.489292</td>\n",
       "      <td>0.210887</td>\n",
       "      <td>0.712530</td>\n",
       "      <td>0.551432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.454917</td>\n",
       "      <td>0.222187</td>\n",
       "      <td>0.713965</td>\n",
       "      <td>0.548250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.439310</td>\n",
       "      <td>0.218878</td>\n",
       "      <td>0.699921</td>\n",
       "      <td>0.481442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.427837</td>\n",
       "      <td>0.251668</td>\n",
       "      <td>0.693655</td>\n",
       "      <td>0.484624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.416832</td>\n",
       "      <td>0.256777</td>\n",
       "      <td>0.700316</td>\n",
       "      <td>0.505832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.427026</td>\n",
       "      <td>0.226322</td>\n",
       "      <td>0.713825</td>\n",
       "      <td>0.554613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.399152</td>\n",
       "      <td>0.536399</td>\n",
       "      <td>0.766983</td>\n",
       "      <td>0.558855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.390228</td>\n",
       "      <td>0.489602</td>\n",
       "      <td>0.757250</td>\n",
       "      <td>0.593849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.371109</td>\n",
       "      <td>0.440912</td>\n",
       "      <td>0.757827</td>\n",
       "      <td>0.628844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.364399</td>\n",
       "      <td>0.566599</td>\n",
       "      <td>0.780583</td>\n",
       "      <td>0.629905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.356046</td>\n",
       "      <td>0.654097</td>\n",
       "      <td>0.805647</td>\n",
       "      <td>0.643690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.362948</td>\n",
       "      <td>0.660036</td>\n",
       "      <td>0.801665</td>\n",
       "      <td>0.681866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.350944</td>\n",
       "      <td>0.662722</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.686108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.349995</td>\n",
       "      <td>0.668027</td>\n",
       "      <td>0.805402</td>\n",
       "      <td>0.673383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345617</td>\n",
       "      <td>0.702457</td>\n",
       "      <td>0.816321</td>\n",
       "      <td>0.692471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.355723</td>\n",
       "      <td>0.571999</td>\n",
       "      <td>0.789590</td>\n",
       "      <td>0.688229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.344522</td>\n",
       "      <td>0.701717</td>\n",
       "      <td>0.816083</td>\n",
       "      <td>0.706257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.342929</td>\n",
       "      <td>0.702874</td>\n",
       "      <td>0.817855</td>\n",
       "      <td>0.708378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.347561</td>\n",
       "      <td>0.700641</td>\n",
       "      <td>0.819197</td>\n",
       "      <td>0.711559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.343771</td>\n",
       "      <td>0.695820</td>\n",
       "      <td>0.816993</td>\n",
       "      <td>0.703075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.342421</td>\n",
       "      <td>0.711525</td>\n",
       "      <td>0.822690</td>\n",
       "      <td>0.700954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.339760</td>\n",
       "      <td>0.722594</td>\n",
       "      <td>0.825855</td>\n",
       "      <td>0.709438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.336733</td>\n",
       "      <td>0.649982</td>\n",
       "      <td>0.805496</td>\n",
       "      <td>0.678685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.337534</td>\n",
       "      <td>0.702355</td>\n",
       "      <td>0.819772</td>\n",
       "      <td>0.710498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.325675</td>\n",
       "      <td>0.720380</td>\n",
       "      <td>0.826093</td>\n",
       "      <td>0.714740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.334887</td>\n",
       "      <td>0.699589</td>\n",
       "      <td>0.817664</td>\n",
       "      <td>0.705196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.330680</td>\n",
       "      <td>0.710384</td>\n",
       "      <td>0.820298</td>\n",
       "      <td>0.704136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.331734</td>\n",
       "      <td>0.708820</td>\n",
       "      <td>0.825807</td>\n",
       "      <td>0.699894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.329565</td>\n",
       "      <td>0.705173</td>\n",
       "      <td>0.825662</td>\n",
       "      <td>0.677625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.321801</td>\n",
       "      <td>0.709902</td>\n",
       "      <td>0.825322</td>\n",
       "      <td>0.709438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.324422</td>\n",
       "      <td>0.719085</td>\n",
       "      <td>0.823939</td>\n",
       "      <td>0.703075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.330452</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.825277</td>\n",
       "      <td>0.710498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.326179</td>\n",
       "      <td>0.723948</td>\n",
       "      <td>0.830498</td>\n",
       "      <td>0.715801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.329725</td>\n",
       "      <td>0.721332</td>\n",
       "      <td>0.828248</td>\n",
       "      <td>0.709438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.329946</td>\n",
       "      <td>0.710505</td>\n",
       "      <td>0.822598</td>\n",
       "      <td>0.713680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.328493</td>\n",
       "      <td>0.716291</td>\n",
       "      <td>0.826764</td>\n",
       "      <td>0.707317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.328058</td>\n",
       "      <td>0.728047</td>\n",
       "      <td>0.830548</td>\n",
       "      <td>0.718982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.319875</td>\n",
       "      <td>0.717336</td>\n",
       "      <td>0.827864</td>\n",
       "      <td>0.708378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.317487</td>\n",
       "      <td>0.698485</td>\n",
       "      <td>0.822067</td>\n",
       "      <td>0.692471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.323930</td>\n",
       "      <td>0.702441</td>\n",
       "      <td>0.822594</td>\n",
       "      <td>0.702015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.320975</td>\n",
       "      <td>0.719205</td>\n",
       "      <td>0.829299</td>\n",
       "      <td>0.712619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.327378</td>\n",
       "      <td>0.706808</td>\n",
       "      <td>0.822451</td>\n",
       "      <td>0.706257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.319309</td>\n",
       "      <td>0.715018</td>\n",
       "      <td>0.826521</td>\n",
       "      <td>0.707317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.321897</td>\n",
       "      <td>0.721464</td>\n",
       "      <td>0.828342</td>\n",
       "      <td>0.706257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.327873</td>\n",
       "      <td>0.714986</td>\n",
       "      <td>0.822694</td>\n",
       "      <td>0.710498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.337722</td>\n",
       "      <td>0.713396</td>\n",
       "      <td>0.824800</td>\n",
       "      <td>0.709438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.325460</td>\n",
       "      <td>0.700226</td>\n",
       "      <td>0.821206</td>\n",
       "      <td>0.700954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.325130</td>\n",
       "      <td>0.706692</td>\n",
       "      <td>0.822930</td>\n",
       "      <td>0.709438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.327654</td>\n",
       "      <td>0.692743</td>\n",
       "      <td>0.819529</td>\n",
       "      <td>0.705196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.329357</td>\n",
       "      <td>0.716471</td>\n",
       "      <td>0.824993</td>\n",
       "      <td>0.711559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.323370</td>\n",
       "      <td>0.715712</td>\n",
       "      <td>0.826428</td>\n",
       "      <td>0.698834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.326575</td>\n",
       "      <td>0.715719</td>\n",
       "      <td>0.823222</td>\n",
       "      <td>0.706257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.324749</td>\n",
       "      <td>0.714712</td>\n",
       "      <td>0.824611</td>\n",
       "      <td>0.697773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.321939</td>\n",
       "      <td>0.717794</td>\n",
       "      <td>0.826717</td>\n",
       "      <td>0.693531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.324404</td>\n",
       "      <td>0.716429</td>\n",
       "      <td>0.824132</td>\n",
       "      <td>0.702015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.325018</td>\n",
       "      <td>0.709647</td>\n",
       "      <td>0.823747</td>\n",
       "      <td>0.702015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.324623</td>\n",
       "      <td>0.707205</td>\n",
       "      <td>0.822071</td>\n",
       "      <td>0.702015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.327959</td>\n",
       "      <td>0.715004</td>\n",
       "      <td>0.822934</td>\n",
       "      <td>0.706257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.324281</td>\n",
       "      <td>0.714744</td>\n",
       "      <td>0.823651</td>\n",
       "      <td>0.704136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.323098</td>\n",
       "      <td>0.717946</td>\n",
       "      <td>0.826093</td>\n",
       "      <td>0.704136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.322976</td>\n",
       "      <td>0.711739</td>\n",
       "      <td>0.824081</td>\n",
       "      <td>0.703075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.324220</td>\n",
       "      <td>0.711209</td>\n",
       "      <td>0.823172</td>\n",
       "      <td>0.703075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.323693</td>\n",
       "      <td>0.714142</td>\n",
       "      <td>0.824465</td>\n",
       "      <td>0.702015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.325115</td>\n",
       "      <td>0.711595</td>\n",
       "      <td>0.823843</td>\n",
       "      <td>0.703075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.325637</td>\n",
       "      <td>0.711268</td>\n",
       "      <td>0.823651</td>\n",
       "      <td>0.702015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-500\n",
      "Configuration saved in /notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-500/config.json\n",
      "Model weights saved in /notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-236] due to args.save_total_limit\n",
      "Deleting older checkpoint [/notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-472] due to args.save_total_limit\n",
      "Deleting older checkpoint [/notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-708] due to args.save_total_limit\n",
      "Deleting older checkpoint [/notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-944] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-1000\n",
      "Configuration saved in /notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-1000/config.json\n",
      "Model weights saved in /notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-1180] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /notebooks/Notebooks/general_notebooks/bert-finetuned-pe-mlc/checkpoint-1000 (score: 0.7149863383079997).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1416, training_loss=0.34909905686890336, metrics={'train_runtime': 1444.8202, 'train_samples_per_second': 15.652, 'train_steps_per_second': 0.98, 'total_flos': 2975050125545472.0, 'train_loss': 0.34909905686890336, 'epoch': 6.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fed72a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.32787325978279114,\n",
       " 'eval_f1': 0.7149863383079997,\n",
       " 'eval_roc_auc': 0.8226943285631305,\n",
       " 'eval_accuracy': 0.7104984093319194,\n",
       " 'eval_runtime': 10.2099,\n",
       " 'eval_samples_per_second': 92.361,\n",
       " 'eval_steps_per_second': 5.779,\n",
       " 'epoch': 6.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69995fd0",
   "metadata": {},
   "source": [
    "### inference on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7be6d7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Using public transportation has a lot of advantages for the modern society facing a lot of problems: the environmental population, the isolation in life, the depletion of natural resources',\n",
       " 'structural_fts_as_text': 'Topic: Public transportation keeps society from the depletion of natural resources, Sentence: Using public transportation has a lot of advantages for the modern society facing a lot of problems: the environmental population, the isolation in life, the depletion of natural resources, Para Number: 5, First in Para: Yes, Last in Para: Yes, Is in Introduction: No, Is in Conclusion: Yes',\n",
       " 'mc': 1,\n",
       " 'cl': 0,\n",
       " 'prem': 0,\n",
       " 'link': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mlc['test'][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d8a93d3",
   "metadata": {
    "id": "3fxjfr8PLD42"
   },
   "outputs": [],
   "source": [
    "text = \"Using public transportation has a lot of advantages for the modern society facing a lot of problems: the environmental population, the isolation in life, the depletion of natural resources\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f8f03b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOBosj4UL2tU",
    "outputId": "be370f49-3840-4c76-b193-76083e49701e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8142ffc6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEkAQleMMT0k",
    "outputId": "fddb51cb-bf01-420a-fd5b-4a7c2e775446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f182d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efae0cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1138\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='143' max='143' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [143/143 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trainer = Trainer(model, data_collator=DataCollatorWithPadding(tokenizer))\n",
    "test_raw_preds, test_labels, _ = test_trainer.predict(encoded_dataset[\"test\"])\n",
    "# test_preds = np.argmax(test_raw_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b258ee89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1138, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ecaa563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd026aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d19d9bc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_raw_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_100/1734734195.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_raw_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_raw_preds' is not defined"
     ]
    }
   ],
   "source": [
    "test_raw_preds[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b92bdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_pred = sigmoid(torch.tensor(test_raw_preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "825eecd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0738, 0.6383, 0.1192, 0.1353])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89c961fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = sigmoid(torch.tensor(test_raw_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb469c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0807, 0.6484, 0.1614, 0.2986],\n",
       "        [0.7678, 0.2760, 0.0419, 0.0443],\n",
       "        [0.0057, 0.1876, 0.8571, 0.3283],\n",
       "        ...,\n",
       "        [0.0104, 0.2795, 0.7419, 0.3969],\n",
       "        [0.0072, 0.1583, 0.8489, 0.2116],\n",
       "        [0.9015, 0.1502, 0.0491, 0.0482]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e53f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice! now do the thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98c9777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros(test_preds.shape)\n",
    "predictions[np.where(test_preds >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "#predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "#print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a9e17f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72ed7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ffa17e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.763     0.964     0.852       140\n",
      "           1      0.666     0.709     0.687       275\n",
      "           2      0.944     0.869     0.905       723\n",
      "           3      0.607     0.596     0.601       267\n",
      "\n",
      "   micro avg      0.800     0.795     0.797      1405\n",
      "   macro avg      0.745     0.784     0.761      1405\n",
      "weighted avg      0.808     0.795     0.799      1405\n",
      " samples avg      0.832     0.816     0.815      1405\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, predictions, digits=3))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5f94f2b",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "           0      0.530     0.436     0.478       140\n",
    "           1      0.421     0.484     0.450       275\n",
    "           2      0.828     0.770     0.798       723\n",
    "           3      0.314     0.120     0.173       267\n",
    "\n",
    "   micro avg      0.649     0.557     0.600      1405\n",
    "   macro avg      0.523     0.452     0.475      1405\n",
    "weighted avg      0.621     0.557     0.579      1405\n",
    " samples avg      0.648     0.607     0.616      1405\n",
    "    \n",
    "    \n",
    "with text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f35d58f5",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "           0      0.787     0.950     0.861       140\n",
    "           1      0.652     0.702     0.676       275\n",
    "           2      0.923     0.884     0.903       723\n",
    "           3      0.588     0.588     0.588       267\n",
    "\n",
    "   micro avg      0.788     0.799     0.793      1405\n",
    "   macro avg      0.738     0.781     0.757      1405\n",
    "weighted avg      0.793     0.799     0.795      1405\n",
    " samples avg      0.823     0.820     0.810      1405\n",
    "    \n",
    "    \n",
    "with strct fts as text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a031725e",
   "metadata": {},
   "source": [
    "## task: do separate classification reports for separate classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d4d0f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec7ec0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da27515e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1138, 4)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "905dc64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1138, 4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "120b82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate test labels into separate task classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "507d56ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first three classes\n",
    "\n",
    "test_labels[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0e12d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linked/not_linked class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25e10fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6395118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1138,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:,3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba2a8f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_comp_classes = test_labels[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "236b5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_link_class = test_labels[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d09194ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1138, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_comp_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7403a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1138,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_link_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1b6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate test predictions into separate task classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2c8ee7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "734286d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c78c81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_comp_classes = predictions[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42219b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_link_class = predictions[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3502d5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1138, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_comp_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "55dd19f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1138,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_link_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62064600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do two separate classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4550e4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.763     0.964     0.852       140\n",
      "           1      0.666     0.709     0.687       275\n",
      "           2      0.944     0.869     0.905       723\n",
      "\n",
      "   micro avg      0.844     0.842     0.843      1138\n",
      "   macro avg      0.791     0.847     0.814      1138\n",
      "weighted avg      0.855     0.842     0.846      1138\n",
      " samples avg      0.839     0.842     0.840      1138\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# for comp classes\n",
    "\n",
    "print(classification_report(test_labels_comp_classes, test_predictions_comp_classes, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c7167ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.877     0.882     0.879       871\n",
      "         1.0      0.607     0.596     0.601       267\n",
      "\n",
      "    accuracy                          0.815      1138\n",
      "   macro avg      0.742     0.739     0.740      1138\n",
      "weighted avg      0.813     0.815     0.814      1138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for link class\n",
    "\n",
    "print(classification_report(test_labels_link_class, test_predictions_link_class, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf6c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the combined strct fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8276096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mlc = torch.load(os.path.join(\"/notebooks/Notebooks/general_notebooks/datasets\", 'pe_dataset_for_MLC.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f68dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_try = dataset_mlc['train']['structural_fts_as_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5992428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Topic: Gender Equality at university admission, Sentence: They want female candidates for soft natured work like counseling, teaching, designing etc, Para Number: 3, First in Para: No, Last in Para: No, Is in Introduction: No, Is in Conclusion: No'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b07a5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_number = str_try.split(\"Para Number: \")[1].split(\",\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb14d1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fb813b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_in_para = str_try.split(\"Para Number: \")[1].split(\",\")[1].split(\" \")[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d287b0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_in_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70d71615",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_in_para = str_try.split(\"Para Number: \")[1].split(\",\")[2].split(\" \")[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0762989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_in_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "366275d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_in_intro = str_try.split(\"Para Number: \")[1].split(\",\")[3].split(\" \")[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1425413e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_in_intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d76eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_in_concl = str_try.split(\"Para Number: \")[1].split(\",\")[4].split(\" \")[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66c47d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_in_concl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a004ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok nice. now concat them to get the combined thing.\n",
    "# see how to do the new dataset column thing in hugging face\n",
    "# then do it for this dataset\n",
    "# then run it on this new combined struct fts sentence representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98d1663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mlc_old = torch.load(os.path.join(\"/notebooks/ICANN/Datasets\", 'dataset_persuasive_essays_icann.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9df2c2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Topic: Should students be taught to compete or to cooperate? Sentence: Consequently, no matter from the view of individual development or the relationship between competition and cooperation we can receive the same conclusion that a more cooperative attitudes towards life is more profitable in one's success. Structural features: Four. No. Yes. Yes. Yes.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mlc_old['train']['topic_full_sentence_structural_fts_combined'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "823ec4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting inflect\n",
      "  Downloading inflect-5.6.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: inflect\n",
      "Successfully installed inflect-5.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1797875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1982c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forty-five'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inflect.engine().number_to_words(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b8406eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_str = \"Structural Features: \" + inflect.engine().number_to_words(para_number).title() + \". \" + first_in_para + \". \" + last_in_para + \". \" + is_in_intro + \". \" + is_in_concl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "94a3b628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Structural Features: Three. No. No. No. No'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create this combined column\n",
    "# then, run the same joint business w 48 batchsize, epoch = 6, learning_rate = 1e-5\n",
    "# check task 3.1 from that other notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
