{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab0b71b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas==1.3.4 in /opt/conda/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.3.4) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.3.4) (1.21.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.3.4) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers==4.12.5 in /opt/conda/lib/python3.8/site-packages (4.12.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (0.8.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (0.0.46)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (2021.10.8)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (3.3.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (0.10.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (1.21.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.12.5) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (2.0.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (8.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets==1.15.1 in /opt/conda/lib/python3.8/site-packages (1.15.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (21.0)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (8.0.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (1.21.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (4.62.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (0.3.5.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (2.26.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (0.70.13)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (1.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (2022.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (0.8.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (3.10.0.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (5.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets==1.15.1) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (1.26.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (6.0.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==1.15.1) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==1.15.1) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.15.1) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (7.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.6.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.28.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.6)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (58.2.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.8/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.1.0)\n",
      "Requirement already satisfied: nbformat in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.11.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.0.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.4.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==1.3.4\n",
    "!pip install transformers==4.12.5\n",
    "!pip install datasets==1.15.1\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7536399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import ClassLabel\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c6937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mlc = torch.load(os.path.join(\"/notebooks/MTL_tasks/data\", 'pe_dataset_for_MLC_w_combined.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c3eb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['essay_nr', 'component_id', 'label_and_comp_idxs', 'text', 'label_ComponentType', 'relation_SupportAttack', 'label_RelationType', 'label_LinkedNotLinked', 'split_y', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'sentence', 'paragraph', 'para_nr', 'total_paras', 'token_count', 'token_count_covering_para', 'tokens_count_covering_sentence', 'preceeding_tokens_in_sentence_count', 'succeeding_tokens_in_sentence_count', 'token_ratio', 'relative_position_in_para_char', 'is_in_intro', 'relative_position_in_para_token', 'is_in_conclusion', 'is_first_in_para', 'is_last_in_para', 'nr_preceeding_comps_in_para', 'nr_following_comps_in_para', 'structural_fts_as_text', 'structural_fts_as_text_combined', 'mc', 'cl', 'prem', 'link_present', 'support', 'attack'],\n",
       "        num_rows: 3770\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['essay_nr', 'component_id', 'label_and_comp_idxs', 'text', 'label_ComponentType', 'relation_SupportAttack', 'label_RelationType', 'label_LinkedNotLinked', 'split_y', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'sentence', 'paragraph', 'para_nr', 'total_paras', 'token_count', 'token_count_covering_para', 'tokens_count_covering_sentence', 'preceeding_tokens_in_sentence_count', 'succeeding_tokens_in_sentence_count', 'token_ratio', 'relative_position_in_para_char', 'is_in_intro', 'relative_position_in_para_token', 'is_in_conclusion', 'is_first_in_para', 'is_last_in_para', 'nr_preceeding_comps_in_para', 'nr_following_comps_in_para', 'structural_fts_as_text', 'structural_fts_as_text_combined', 'mc', 'cl', 'prem', 'link_present', 'support', 'attack'],\n",
       "        num_rows: 1260\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['essay_nr', 'component_id', 'label_and_comp_idxs', 'text', 'label_ComponentType', 'relation_SupportAttack', 'label_RelationType', 'label_LinkedNotLinked', 'split_y', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'sentence', 'paragraph', 'para_nr', 'total_paras', 'token_count', 'token_count_covering_para', 'tokens_count_covering_sentence', 'preceeding_tokens_in_sentence_count', 'succeeding_tokens_in_sentence_count', 'token_ratio', 'relative_position_in_para_char', 'is_in_intro', 'relative_position_in_para_token', 'is_in_conclusion', 'is_first_in_para', 'is_last_in_para', 'nr_preceeding_comps_in_para', 'nr_following_comps_in_para', 'structural_fts_as_text', 'structural_fts_as_text_combined', 'mc', 'cl', 'prem', 'link_present', 'support', 'attack'],\n",
       "        num_rows: 943\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b503bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do we need: 'text', 'structural_fts_as_text', 'structural_fts_as_text_combined' 'mc', 'cl', 'prem', 'link'\n",
    "# that's enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed748b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_not_needed = ['essay_nr', 'component_id', 'label_and_comp_idxs', 'label_ComponentType', 'relation_SupportAttack', 'label_RelationType', 'label_LinkedNotLinked', 'split_y', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'sentence', 'paragraph', 'para_nr', 'total_paras', 'token_count', 'token_count_covering_para', 'tokens_count_covering_sentence', 'preceeding_tokens_in_sentence_count', 'succeeding_tokens_in_sentence_count', 'token_ratio', 'relative_position_in_para_char', 'is_in_intro', 'relative_position_in_para_token', 'is_in_conclusion', 'is_first_in_para', 'is_last_in_para', 'nr_preceeding_comps_in_para', 'nr_following_comps_in_para']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f87e13c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mlc = dataset_mlc.remove_columns(cols_not_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5343155",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mlc = dataset_mlc.remove_columns(['support', 'attack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa1d1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'structural_fts_as_text', 'structural_fts_as_text_combined', 'mc', 'cl', 'prem', 'link_present'],\n",
       "        num_rows: 3770\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'structural_fts_as_text', 'structural_fts_as_text_combined', 'mc', 'cl', 'prem', 'link_present'],\n",
       "        num_rows: 1260\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'structural_fts_as_text', 'structural_fts_as_text_combined', 'mc', 'cl', 'prem', 'link_present'],\n",
       "        num_rows: 943\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9fde889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'we cannot ignore its negative effects',\n",
       " 'structural_fts_as_text': 'Topic: Advantages and disadvantages of machines instead of human to do the work, Sentence: In my opinion, although using machines have many benefits, we cannot ignore its negative effects., Para Number: 1, First in Para: No, Last in Para: Yes, Is in Introduction: Yes, Is in Conclusion: No',\n",
       " 'structural_fts_as_text_combined': 'Topic: Advantages and disadvantages of machines instead of human to do the work, Sentence: In my opinion, although using machines have many benefits, we cannot ignore its negative effects., Structural Features: 1, No, Yes, Yes, No',\n",
       " 'mc': 1,\n",
       " 'cl': 0,\n",
       " 'prem': 0,\n",
       " 'link_present': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mlc['train'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49177b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mc', 'cl', 'prem', 'link_present']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset_mlc['train'].features.keys() if label not in ['text', 'structural_fts_as_text', 'structural_fts_as_text_combined']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c39406b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "#from transformers import Trainer\n",
    "from transformers import AutoTokenizer\n",
    "#from transformers import BertForSequenceClassification\n",
    "#from transformers import Trainer, TrainingArguments\n",
    "#from transformers.data.data_collator import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb660db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"structural_fts_as_text_combined\"]\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256)\n",
    "    # add labels\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    # fill numpy array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "        encoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56f3432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_data at 0x7ff1611ecca0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca507b08e344446ca548eadbefadd433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a54713cdc314207a8c760bb11e863dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2473683568e4b3ca0eacefb3d00c24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset_mlc.map(preprocess_data, batched=True, remove_columns=dataset_mlc['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbff5557",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0enAb0W9o25W",
    "outputId": "55bc5ba6-d169-49c6-f562-bb7ea4143866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['attention_mask', 'input_ids', 'labels', 'token_type_ids'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64e8c16e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "D0McCtJ8HRJY",
    "outputId": "82fb0336-51a3-40ad-ebc0-65eeb7cf4b6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] topic : gender equality at university admission, sentence : therefore, universities follow the requirement of job providers and decide subject suitable for particular gender., structural features : 3, no, yes, no, no [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "277502ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdIvj6WjHeZQ",
    "outputId": "418c14d2-cca3-44e9-d0a2-6ad4ca7a007c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1dba313",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4Dx95t2o6N9",
    "outputId": "3ce6c923-0b45-4743-bc4b-7771d088b03e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cl', 'link_present']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8b79c70",
   "metadata": {
    "id": "Lk6Cq9duKBkA"
   },
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3563ce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc0b7fd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a78b2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe431937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "939c4e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dae97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "metric_name = \"f1\"\n",
    "nr_epochs = 6\n",
    "results_folder = \"/notebooks/MTL_tasks/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a4ee753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \n",
    "    output_dir=results_folder,          \n",
    "    \n",
    "    #params\n",
    "    num_train_epochs=nr_epochs,               # nb of epochs\n",
    "    per_device_train_batch_size=batch_size,   # batch size per device during training\n",
    "    per_device_eval_batch_size=batch_size,    # cf. paper Sun et al.\n",
    "    learning_rate=1e-5,#2e-5,                 # cf. paper Sun et al.\n",
    "#     warmup_steps=500,                         # number of warmup steps for learning rate scheduler\n",
    "    warmup_ratio=0.1,                         # cf. paper Sun et al.\n",
    "    weight_decay=0.01,                        # strength of weight decay\n",
    "    \n",
    "    #eval\n",
    "    evaluation_strategy=\"steps\",              # cf. paper Sun et al.\n",
    "    eval_steps=20,                            # cf. paper Sun et al.\n",
    "    \n",
    "    # log\n",
    "#     logging_dir=\"/notebooks/Results/bert_sequence_classification/tb_logs\",  \n",
    "#     logging_strategy='steps',\n",
    "#     logging_steps=20,\n",
    "    \n",
    "    # save\n",
    "    save_strategy='steps',\n",
    "    save_total_limit=2,\n",
    "    #save_steps=20, # default 500\n",
    "    load_best_model_at_end=True,              # cf. paper Sun et al.\n",
    "    # metric_for_best_model='eval_loss' \n",
    "    metric_for_best_model=metric_name\n",
    "    \n",
    "    \n",
    "    \n",
    "#     f\"bert-finetuned-pe-mlc\",\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "#     save_strategy = \"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=metric_name,\n",
    "#     #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45cd47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_macro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "431dff74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IlOgGiojuWwG",
    "outputId": "cd0b2c99-b520-468d-8ffc-c36211b7820a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12901c7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y41Kre_jvD7x",
    "outputId": "b6ca888b-6371-40fb-ab83-3dc24d28320a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  8476,  1024,  5907,  9945,  2012,  2118,  9634,  1010,  6251,\n",
       "         1024,  3568,  1010,  5534,  3582,  1996,  9095,  1997,  3105, 11670,\n",
       "         1998,  5630,  3395,  7218,  2005,  3327,  5907,  1012,  1010,  8332,\n",
       "         2838,  1024,  1017,  1010,  2053,  1010,  2748,  1010,  2053,  1010,\n",
       "         2053,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f317cbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxWcnZ8ku12V",
    "outputId": "26522911-c3cd-466a-ae2d-d81d23003c23"
   },
   "outputs": [],
   "source": [
    "#forward pass\n",
    "# outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55d2c1c0",
   "metadata": {
    "id": "chq_3nUz73ib"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65d9ec4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KXmFds8js6P8",
    "outputId": "66ebb2ab-f93f-48aa-a4dc-36f55f2f8559",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3770\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1416' max='1416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1416/1416 22:43, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.616356</td>\n",
       "      <td>0.188984</td>\n",
       "      <td>0.640430</td>\n",
       "      <td>0.522800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.581645</td>\n",
       "      <td>0.189987</td>\n",
       "      <td>0.642065</td>\n",
       "      <td>0.522800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.520545</td>\n",
       "      <td>0.403593</td>\n",
       "      <td>0.700057</td>\n",
       "      <td>0.452810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.497388</td>\n",
       "      <td>0.421229</td>\n",
       "      <td>0.712257</td>\n",
       "      <td>0.520679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.476716</td>\n",
       "      <td>0.495992</td>\n",
       "      <td>0.736699</td>\n",
       "      <td>0.593849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.464970</td>\n",
       "      <td>0.530684</td>\n",
       "      <td>0.749638</td>\n",
       "      <td>0.632025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.557834</td>\n",
       "      <td>0.763622</td>\n",
       "      <td>0.601273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.450329</td>\n",
       "      <td>0.556003</td>\n",
       "      <td>0.759919</td>\n",
       "      <td>0.559915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.425393</td>\n",
       "      <td>0.553727</td>\n",
       "      <td>0.762536</td>\n",
       "      <td>0.617179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.424346</td>\n",
       "      <td>0.533110</td>\n",
       "      <td>0.752891</td>\n",
       "      <td>0.605514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.423794</td>\n",
       "      <td>0.522421</td>\n",
       "      <td>0.746662</td>\n",
       "      <td>0.586426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.409994</td>\n",
       "      <td>0.655816</td>\n",
       "      <td>0.781900</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.410490</td>\n",
       "      <td>0.664186</td>\n",
       "      <td>0.779941</td>\n",
       "      <td>0.650053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.670314</td>\n",
       "      <td>0.784670</td>\n",
       "      <td>0.651113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.406944</td>\n",
       "      <td>0.602253</td>\n",
       "      <td>0.760083</td>\n",
       "      <td>0.607635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.401515</td>\n",
       "      <td>0.696005</td>\n",
       "      <td>0.794806</td>\n",
       "      <td>0.662778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.402006</td>\n",
       "      <td>0.686720</td>\n",
       "      <td>0.791349</td>\n",
       "      <td>0.648993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.391685</td>\n",
       "      <td>0.698053</td>\n",
       "      <td>0.797918</td>\n",
       "      <td>0.664899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.380021</td>\n",
       "      <td>0.721308</td>\n",
       "      <td>0.807059</td>\n",
       "      <td>0.681866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.383850</td>\n",
       "      <td>0.693448</td>\n",
       "      <td>0.801099</td>\n",
       "      <td>0.618240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.380065</td>\n",
       "      <td>0.715490</td>\n",
       "      <td>0.806531</td>\n",
       "      <td>0.679745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.373714</td>\n",
       "      <td>0.724706</td>\n",
       "      <td>0.810939</td>\n",
       "      <td>0.689290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.381944</td>\n",
       "      <td>0.719573</td>\n",
       "      <td>0.808285</td>\n",
       "      <td>0.688229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.369553</td>\n",
       "      <td>0.725085</td>\n",
       "      <td>0.812778</td>\n",
       "      <td>0.670201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.393658</td>\n",
       "      <td>0.686144</td>\n",
       "      <td>0.794905</td>\n",
       "      <td>0.662778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.401148</td>\n",
       "      <td>0.680951</td>\n",
       "      <td>0.796010</td>\n",
       "      <td>0.623542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.373440</td>\n",
       "      <td>0.703031</td>\n",
       "      <td>0.800075</td>\n",
       "      <td>0.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.378391</td>\n",
       "      <td>0.724795</td>\n",
       "      <td>0.807932</td>\n",
       "      <td>0.672322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.365008</td>\n",
       "      <td>0.723086</td>\n",
       "      <td>0.812842</td>\n",
       "      <td>0.672322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.369454</td>\n",
       "      <td>0.726492</td>\n",
       "      <td>0.811284</td>\n",
       "      <td>0.689290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.380873</td>\n",
       "      <td>0.692498</td>\n",
       "      <td>0.797027</td>\n",
       "      <td>0.664899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.366479</td>\n",
       "      <td>0.732012</td>\n",
       "      <td>0.814195</td>\n",
       "      <td>0.678685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.367564</td>\n",
       "      <td>0.728102</td>\n",
       "      <td>0.812246</td>\n",
       "      <td>0.687169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.371650</td>\n",
       "      <td>0.723892</td>\n",
       "      <td>0.810238</td>\n",
       "      <td>0.688229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.362603</td>\n",
       "      <td>0.734656</td>\n",
       "      <td>0.813804</td>\n",
       "      <td>0.682927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.376134</td>\n",
       "      <td>0.676379</td>\n",
       "      <td>0.790412</td>\n",
       "      <td>0.628844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.360939</td>\n",
       "      <td>0.721022</td>\n",
       "      <td>0.813650</td>\n",
       "      <td>0.679745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.358270</td>\n",
       "      <td>0.730492</td>\n",
       "      <td>0.814654</td>\n",
       "      <td>0.687169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.356536</td>\n",
       "      <td>0.738442</td>\n",
       "      <td>0.816853</td>\n",
       "      <td>0.687169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.372843</td>\n",
       "      <td>0.729360</td>\n",
       "      <td>0.813413</td>\n",
       "      <td>0.690350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.370857</td>\n",
       "      <td>0.729911</td>\n",
       "      <td>0.813364</td>\n",
       "      <td>0.693531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.355419</td>\n",
       "      <td>0.736837</td>\n",
       "      <td>0.818770</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.365939</td>\n",
       "      <td>0.710934</td>\n",
       "      <td>0.812550</td>\n",
       "      <td>0.692471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.368382</td>\n",
       "      <td>0.739328</td>\n",
       "      <td>0.817694</td>\n",
       "      <td>0.691410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.359226</td>\n",
       "      <td>0.731342</td>\n",
       "      <td>0.813822</td>\n",
       "      <td>0.680806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.353952</td>\n",
       "      <td>0.744869</td>\n",
       "      <td>0.821046</td>\n",
       "      <td>0.698834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.357423</td>\n",
       "      <td>0.744439</td>\n",
       "      <td>0.820433</td>\n",
       "      <td>0.693531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.362879</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.820074</td>\n",
       "      <td>0.694592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.356601</td>\n",
       "      <td>0.754713</td>\n",
       "      <td>0.826621</td>\n",
       "      <td>0.700954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.353754</td>\n",
       "      <td>0.731649</td>\n",
       "      <td>0.817734</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.356392</td>\n",
       "      <td>0.743733</td>\n",
       "      <td>0.821659</td>\n",
       "      <td>0.703075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.353606</td>\n",
       "      <td>0.746131</td>\n",
       "      <td>0.822509</td>\n",
       "      <td>0.700954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.356453</td>\n",
       "      <td>0.744824</td>\n",
       "      <td>0.823013</td>\n",
       "      <td>0.702015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.355623</td>\n",
       "      <td>0.751145</td>\n",
       "      <td>0.822205</td>\n",
       "      <td>0.692471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.360058</td>\n",
       "      <td>0.751252</td>\n",
       "      <td>0.822741</td>\n",
       "      <td>0.704136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.360398</td>\n",
       "      <td>0.743672</td>\n",
       "      <td>0.820546</td>\n",
       "      <td>0.698834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.357510</td>\n",
       "      <td>0.751456</td>\n",
       "      <td>0.825053</td>\n",
       "      <td>0.697773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.355271</td>\n",
       "      <td>0.749657</td>\n",
       "      <td>0.824426</td>\n",
       "      <td>0.702015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.354324</td>\n",
       "      <td>0.750664</td>\n",
       "      <td>0.823573</td>\n",
       "      <td>0.693531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.359251</td>\n",
       "      <td>0.751368</td>\n",
       "      <td>0.823809</td>\n",
       "      <td>0.700954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.358174</td>\n",
       "      <td>0.750734</td>\n",
       "      <td>0.823745</td>\n",
       "      <td>0.697773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.355600</td>\n",
       "      <td>0.748508</td>\n",
       "      <td>0.824313</td>\n",
       "      <td>0.696713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.358365</td>\n",
       "      <td>0.751305</td>\n",
       "      <td>0.825857</td>\n",
       "      <td>0.710498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.356913</td>\n",
       "      <td>0.750546</td>\n",
       "      <td>0.825303</td>\n",
       "      <td>0.704136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.356345</td>\n",
       "      <td>0.750148</td>\n",
       "      <td>0.826297</td>\n",
       "      <td>0.721103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.357671</td>\n",
       "      <td>0.742636</td>\n",
       "      <td>0.822072</td>\n",
       "      <td>0.708378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.355618</td>\n",
       "      <td>0.750799</td>\n",
       "      <td>0.825748</td>\n",
       "      <td>0.712619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.356574</td>\n",
       "      <td>0.749879</td>\n",
       "      <td>0.825776</td>\n",
       "      <td>0.705196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.358090</td>\n",
       "      <td>0.750579</td>\n",
       "      <td>0.826089</td>\n",
       "      <td>0.705196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.358205</td>\n",
       "      <td>0.751674</td>\n",
       "      <td>0.826466</td>\n",
       "      <td>0.705196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /notebooks/MTL_tasks/results/checkpoint-500\n",
      "Configuration saved in /notebooks/MTL_tasks/results/checkpoint-500/config.json\n",
      "Model weights saved in /notebooks/MTL_tasks/results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /notebooks/MTL_tasks/results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /notebooks/MTL_tasks/results/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /notebooks/MTL_tasks/results/checkpoint-1000\n",
      "Configuration saved in /notebooks/MTL_tasks/results/checkpoint-1000/config.json\n",
      "Model weights saved in /notebooks/MTL_tasks/results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /notebooks/MTL_tasks/results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /notebooks/MTL_tasks/results/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /notebooks/MTL_tasks/results/checkpoint-1000 (score: 0.7316491358231823).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1416, training_loss=0.3630037738778497, metrics={'train_runtime': 1365.1621, 'train_samples_per_second': 16.569, 'train_steps_per_second': 1.037, 'total_flos': 2975839472885760.0, 'train_loss': 0.3630037738778497, 'epoch': 6.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fe52842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3537537753582001,\n",
       " 'eval_f1': 0.7316491358231823,\n",
       " 'eval_roc_auc': 0.817733943064615,\n",
       " 'eval_accuracy': 0.6956521739130435,\n",
       " 'eval_runtime': 9.8135,\n",
       " 'eval_samples_per_second': 96.092,\n",
       " 'eval_steps_per_second': 6.012,\n",
       " 'epoch': 6.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a27ed2",
   "metadata": {},
   "source": [
    "### inference on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e06f74ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'the individual should finance his or her education',\n",
       " 'structural_fts_as_text': 'Topic: Should the Government Provide Free College?, Sentence: In short, the individual should finance his or her education because it is a personal choice, Para Number: 5, First in Para: Yes, Last in Para: No, Is in Introduction: No, Is in Conclusion: Yes',\n",
       " 'structural_fts_as_text_combined': 'Topic: Should the Government Provide Free College?, Sentence: In short, the individual should finance his or her education because it is a personal choice, Structural Features: 5, Yes, No, No, Yes',\n",
       " 'mc': 1,\n",
       " 'cl': 0,\n",
       " 'prem': 0,\n",
       " 'link_present': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mlc['test'][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "330bae40",
   "metadata": {
    "id": "3fxjfr8PLD42"
   },
   "outputs": [],
   "source": [
    "text = \"Using public transportation has a lot of advantages for the modern society facing a lot of problems: the environmental population, the isolation in life, the depletion of natural resources\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66620e94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOBosj4UL2tU",
    "outputId": "be370f49-3840-4c76-b193-76083e49701e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77b39756",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEkAQleMMT0k",
    "outputId": "fddb51cb-bf01-420a-fd5b-4a7c2e775446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prem']\n"
     ]
    }
   ],
   "source": [
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0942753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82fa816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1260\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trainer = Trainer(model, data_collator=DataCollatorWithPadding(tokenizer))\n",
    "test_raw_preds, test_labels, _ = test_trainer.predict(encoded_dataset[\"test\"])\n",
    "# test_preds = np.argmax(test_raw_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d4d37fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 4)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "daf4312f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a0bf14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38646b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.845425 , -2.4463947,  2.5750885, -1.5848606], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw_preds[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "419e3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_pred = sigmoid(torch.tensor(test_raw_preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9247298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6311, 0.3490, 0.0473, 0.6861])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd9482f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = sigmoid(torch.tensor(test_raw_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "310bce32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6311, 0.3490, 0.0473, 0.6861],\n",
       "        [0.8102, 0.2365, 0.0459, 0.7859],\n",
       "        [0.0689, 0.5003, 0.1705, 0.6263],\n",
       "        ...,\n",
       "        [0.0083, 0.0794, 0.9330, 0.1557],\n",
       "        [0.0142, 0.0548, 0.9538, 0.1742],\n",
       "        [0.0134, 0.0602, 0.9541, 0.1601]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11083425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice! now do the thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be7a8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros(test_preds.shape)\n",
    "predictions[np.where(test_preds >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "#predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "#print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea72af1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1.],\n",
       "       [0., 1., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1ce02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6d5a036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.770     0.941     0.847       153\n",
      "           1      0.730     0.546     0.625       302\n",
      "           2      0.898     0.918     0.908       805\n",
      "           3      0.750     0.657     0.700       516\n",
      "\n",
      "   micro avg      0.822     0.781     0.801      1776\n",
      "   macro avg      0.787     0.766     0.770      1776\n",
      "weighted avg      0.815     0.781     0.794      1776\n",
      " samples avg      0.834     0.801     0.808      1776\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, predictions, digits=3))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90f795bb",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "           0      0.649     0.314     0.423       153\n",
    "           1      0.405     0.354     0.378       302\n",
    "           2      0.791     0.832     0.811       805\n",
    "           3      0.659     0.566     0.609       516\n",
    "\n",
    "   micro avg      0.686     0.629     0.656      1776\n",
    "   macro avg      0.626     0.517     0.555      1776\n",
    "weighted avg      0.675     0.629     0.645      1776\n",
    " samples avg      0.718     0.679     0.683      1776\n",
    "    \n",
    "    \n",
    "with text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3ff8831",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "           0      0.771     0.967     0.858       153\n",
    "           1      0.666     0.672     0.669       302\n",
    "           2      0.914     0.887     0.900       805\n",
    "           3      0.743     0.702     0.722       516\n",
    "\n",
    "   micro avg      0.808     0.803     0.806      1776\n",
    "   macro avg      0.773     0.807     0.787      1776\n",
    "weighted avg      0.810     0.803     0.805      1776\n",
    " samples avg      0.843     0.815     0.819      1776\n",
    "    \n",
    "with strct fts as txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b311728",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "           0      0.770     0.941     0.847       153\n",
    "           1      0.730     0.546     0.625       302\n",
    "           2      0.898     0.918     0.908       805\n",
    "           3      0.750     0.657     0.700       516\n",
    "\n",
    "   micro avg      0.822     0.781     0.801      1776\n",
    "   macro avg      0.787     0.766     0.770      1776\n",
    "weighted avg      0.815     0.781     0.794      1776\n",
    " samples avg      0.834     0.801     0.808      1776\n",
    "    \n",
    "with strcts fts as text combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee606c",
   "metadata": {},
   "source": [
    "## task: do separate classification reports for separate classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "249324dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c7ce0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1.],\n",
       "       [0., 1., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c102027d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "36a8a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7988b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate test labels into separate task classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1fb1f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first three classes\n",
    "\n",
    "test_labels[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ff6373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linked/not_linked class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dda51544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b80c0f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:,3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e4a616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_comp_classes = test_labels[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dcd54824",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_link_class = test_labels[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07306d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_comp_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb53c482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_link_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e614ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate test predictions into separate task classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02ec81ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6acf718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c84066e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_comp_classes = predictions[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4e00390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_link_class = predictions[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b65846c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_comp_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1fce2795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_link_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08ea4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do two separate classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1bf0d934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.770     0.941     0.847       153\n",
      "           1      0.730     0.546     0.625       302\n",
      "           2      0.898     0.918     0.908       805\n",
      "\n",
      "   micro avg      0.848     0.832     0.840      1260\n",
      "   macro avg      0.799     0.802     0.793      1260\n",
      "weighted avg      0.842     0.832     0.833      1260\n",
      " samples avg      0.829     0.832     0.830      1260\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# for comp classes\n",
    "\n",
    "print(classification_report(test_labels_comp_classes, test_predictions_comp_classes, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e53935ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.781     0.848     0.813       744\n",
      "         1.0      0.750     0.657     0.700       516\n",
      "\n",
      "    accuracy                          0.770      1260\n",
      "   macro avg      0.765     0.753     0.757      1260\n",
      "weighted avg      0.768     0.770     0.767      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for link class\n",
    "\n",
    "print(classification_report(test_labels_link_class, test_predictions_link_class, digits=3))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5c96c1c",
   "metadata": {},
   "source": [
    "with text:\n",
    "    \n",
    "precision    recall  f1-score   support\n",
    "\n",
    "           0      0.649     0.314     0.423       153\n",
    "           1      0.405     0.354     0.378       302\n",
    "           2      0.791     0.832     0.811       805\n",
    "\n",
    "   micro avg      0.696     0.655     0.675      1260\n",
    "   macro avg      0.615     0.500     0.537      1260\n",
    "weighted avg      0.681     0.655     0.660      1260\n",
    " samples avg      0.632     0.655     0.640      1260\n",
    " \n",
    " \n",
    "  precision    recall  f1-score   support\n",
    "\n",
    "         0.0      0.726     0.797     0.760       744\n",
    "         1.0      0.659     0.566     0.609       516\n",
    "\n",
    "    accuracy                          0.702      1260\n",
    "   macro avg      0.692     0.681     0.684      1260\n",
    "weighted avg      0.699     0.702     0.698      1260"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a148c4f5",
   "metadata": {},
   "source": [
    "with strct fts as text\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "           0      0.771     0.967     0.858       153\n",
    "           1      0.666     0.672     0.669       302\n",
    "           2      0.914     0.887     0.900       805\n",
    "\n",
    "   micro avg      0.833     0.845     0.839      1260\n",
    "   macro avg      0.784     0.842     0.809      1260\n",
    "weighted avg      0.837     0.845     0.840      1260\n",
    " samples avg      0.833     0.845     0.837      1260\n",
    "    \n",
    "precision    recall  f1-score   support\n",
    "\n",
    "         0.0      0.801     0.832     0.816       744\n",
    "         1.0      0.743     0.702     0.722       516\n",
    "\n",
    "    accuracy                          0.779      1260\n",
    "   macro avg      0.772     0.767     0.769      1260\n",
    "weighted avg      0.777     0.779     0.777      1260"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff4b3595",
   "metadata": {},
   "source": [
    "with strct fts as text combined:\n",
    "    \n",
    "precision    recall  f1-score   support\n",
    "\n",
    "           0      0.770     0.941     0.847       153\n",
    "           1      0.730     0.546     0.625       302\n",
    "           2      0.898     0.918     0.908       805\n",
    "\n",
    "   micro avg      0.848     0.832     0.840      1260\n",
    "   macro avg      0.799     0.802     0.793      1260\n",
    "weighted avg      0.842     0.832     0.833      1260\n",
    " samples avg      0.829     0.832     0.830      1260\n",
    "    \n",
    "    \n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "         0.0      0.781     0.848     0.813       744\n",
    "         1.0      0.750     0.657     0.700       516\n",
    "\n",
    "    accuracy                          0.770      1260\n",
    "   macro avg      0.765     0.753     0.757      1260\n",
    "weighted avg      0.768     0.770     0.767      1260"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
